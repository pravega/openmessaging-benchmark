{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pylab as plt\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "import json\n",
    "from io import StringIO\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import molten_data_common_lib\n",
    "importlib.reload(molten_data_common_lib)\n",
    "from molten_data_common_lib import (glob_file_list , load_json_from_file, merge_dicts, plot_groups, \n",
    "                                    get_varying_column_names, filter_dataframe, take_varying_columns,\n",
    "                                    load_json_records_as_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load result files from P3 Test Driver\n",
    "src_files = []\n",
    "src_files += ['../data/p3_test_driver/results/*.json']\n",
    "raw_df = load_json_records_as_dataframe(src=src_files, ignore_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean raw results\n",
    "def clean_result(result):\n",
    "    try:\n",
    "        r = result.copy()\n",
    "        r['utc_begin'] = pd.to_datetime(r['utc_begin'], utc=True)\n",
    "        r['utc_end'] = pd.to_datetime(r['utc_end'], utc=True)\n",
    "        r['git_commit'] = r['git_commit'].split(' ')[0]\n",
    "        r['driverName'] = r['driver']['name']\n",
    "        if r['driverName'] == 'Pulsar':\n",
    "            r = merge_dicts(r, r['driver']['client']['persistence'])\n",
    "        r = merge_dicts(r, r['workload'])\n",
    "        del r['workload']\n",
    "        r = merge_dicts(r, r['omb_results'])\n",
    "        if 'ansible_vars' in r and isinstance(r['ansible_vars'], dict):\n",
    "            r = merge_dicts(r, r['ansible_vars'])\n",
    "        if r['driverName'] == 'Pravega':\n",
    "            if 'pravegaVersion' not in r:\n",
    "                r['pravegaVersion'] = '0.6.0-2361.f273314-SNAPSHOT'\n",
    "            r['pravegaVersion'] = r['pravegaVersion'].replace('-SNAPSHOT','')\n",
    "        for k in list(r.keys()):\n",
    "            if 'Quantiles' in k:\n",
    "                r[k] = pd.Series(data=[float(q) for q in r[k].keys()], index=list(r[k].values())).sort_index() / 100\n",
    "            elif isinstance(r[k], list) and 'Rate' in k:\n",
    "                r[k] = pd.Series(r[k])\n",
    "                r['%sMean' % k] = r[k].mean()\n",
    "        r['numWorkloadWorkers'] = int(r.get('numWorkers', 0))\n",
    "        r['throttleEventsPerSec'] = r['producerRate']\n",
    "        r['publishRateEventsPerSecMean'] = r['publishRateMean']\n",
    "        r['publishRateMBPerSecMean'] = r['publishRateMean'] * r['messageSize'] * 1e-6\n",
    "        r['publishLatencyMsAvg'] = r['aggregatedPublishLatencyAvg']\n",
    "        r['publishLatencyMs50Pct'] = r['aggregatedPublishLatency50pct']\n",
    "        r['publishLatencyMs99Pct'] = r['aggregatedPublishLatency99pct']\n",
    "        r['endToEndLatencyMsAvg'] = r['aggregatedEndToEndLatencyAvg']\n",
    "        r['endToEndLatencyMs50Pct'] = r['aggregatedEndToEndLatency50pct']\n",
    "        r['endToEndLatencyMs99Pct'] = r['aggregatedEndToEndLatency99pct']\n",
    "        return pd.Series(r)\n",
    "    except Exception as e:\n",
    "        print('ERROR: %s: %s' % (r['test_uuid'], e))\n",
    "        # raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r = clean_result(raw_df.iloc[-1])\n",
    "# pd.DataFrame(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = raw_df.apply(clean_result, axis=1)\n",
    "clean_df = clean_df.set_index('test_uuid', drop=False)\n",
    "clean_df = clean_df[clean_df.error==False]\n",
    "clean_df = clean_df.sort_values(['utc_begin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show list of columns\n",
    "clean_df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns that identify test parameters\n",
    "param_cols = [\n",
    "    'numWorkloadWorkers',\n",
    "    'topics',\n",
    "    'partitionsPerTopic',\n",
    "    'producersPerTopic',\n",
    "    'subscriptionsPerTopic',\n",
    "    'consumerPerSubscription',\n",
    "    'testDurationMinutes',\n",
    "    'keyDistributor',\n",
    "    'git_commit',\n",
    "    'pravegaVersion',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns that are the output of the experiments\n",
    "output_cols = [\n",
    "    'publishRateEventsPerSecMean',\n",
    "    'publishRateMBPerSecMean',\n",
    "    'publishLatencyMs50Pct',\n",
    "    'publishLatencyMs99Pct',\n",
    "    'endToEndLatencyMs50Pct',\n",
    "    'endToEndLatencyMs99Pct',\n",
    "    'utc_begin',    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = param_cols + output_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View most recent results\n",
    "clean_df[cols].tail(3).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to CSV\n",
    "#clean_df[cols].to_csv('openmessaging-benchmark-results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = clean_df[cols]\n",
    "# df = df.sort_values(['messageSize','numWorkloadWorkers','producersPerTopic','throttleEventsPerSec','utc_begin'])\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View distinct values of pravegaVersion and test counts\n",
    "clean_df.groupby(['pravegaVersion']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First level of filtering\n",
    "filt_df = filter_dataframe(\n",
    "    clean_df,\n",
    "    driverName='Pravega',\n",
    "#     pravegaVersion='0.6.0-2361.f273314',\n",
    "    pravegaVersion='0.6.0-2386.23b7340',\n",
    "    numWorkloadWorkers=4, \n",
    "    topics=1,\n",
    "    testDurationMinutes=15,\n",
    "    size_of_test_batch=(2,1000), # between\n",
    "    aggregatedEndToEndLatency50pct=(1,1e6),\n",
    ")\n",
    "# filt_df = filt_df[filt_df.size_of_test_batch > 1]\n",
    "len(filt_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latency_vs_throughput_table(df):\n",
    "    result_df = (df\n",
    "        .set_index(['publishRateMBPerSecMean'])\n",
    "        .sort_index()\n",
    "        [[\n",
    "            'aggregatedPublishLatency50pct',\n",
    "            'aggregatedPublishLatency95pct',\n",
    "            'aggregatedPublishLatency99pct',\n",
    "            'aggregatedEndToEndLatency50pct',\n",
    "            'aggregatedEndToEndLatency95pct',\n",
    "            'aggregatedEndToEndLatency99pct',\n",
    "            'test_uuid',\n",
    "        ]]\n",
    "        .rename(columns=dict(\n",
    "            aggregatedPublishLatency50pct='Publish Latency p50',\n",
    "            aggregatedPublishLatency95pct='Publish Latency p95',\n",
    "            aggregatedPublishLatency99pct='Publish Latency p99',\n",
    "            aggregatedEndToEndLatency50pct='E2E Latency p50',\n",
    "            aggregatedEndToEndLatency95pct='E2E Latency p95',\n",
    "            aggregatedEndToEndLatency99pct='E2E Latency p99',\n",
    "        ))\n",
    "        )\n",
    "    result_df.index.name = 'Publish Throughput (MB/s)'\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_latency_vs_throughput(df):\n",
    "    assert len(df.messageSize.drop_duplicates().values) == 1\n",
    "    messageSize = df.messageSize.iloc[0]\n",
    "    plot_df = latency_vs_throughput_table(df)\n",
    "    title = 'Message Size %d' % (messageSize)\n",
    "    ax = plot_df.plot(\n",
    "        logx=True, \n",
    "        logy=True,\n",
    "        figsize=(10,8),\n",
    "        grid=True, \n",
    "        title=title, \n",
    "        style=['x:b','x-.b','x-b','+:r','+-.r','+-r'])\n",
    "    ax.set_ylabel('Latency (ms)');\n",
    "    tick_formatter = matplotlib.ticker.LogFormatter()\n",
    "    ax.xaxis.set_major_formatter(tick_formatter)\n",
    "    ax.yaxis.set_major_formatter(tick_formatter)\n",
    "    ax.grid('on', which='both', axis='both')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Message Size 100 B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt_100_df = filter_dataframe(\n",
    "    filt_df,\n",
    "    messageSize=100,\n",
    "    producersPerTopic=32,\n",
    "    partitionsPerTopic=16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View varying columns\n",
    "take_varying_columns(filt_100_df[filt_100_df.producerRate==100000]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View distinct sets of parameters.\n",
    "# There should only be one distinct set of parameters.\n",
    "filt_100_df[param_cols].drop_duplicates().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_latency_vs_throughput(filt_100_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latency_vs_throughput_table(filt_100_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Message Size 10 KB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt_10000_df = filter_dataframe(\n",
    "    filt_df,\n",
    "    messageSize=10000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View distinct sets of parameters.\n",
    "# There should only be one distinct set of parameters.\n",
    "filt_10000_df[param_cols].drop_duplicates().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_latency_vs_throughput(filt_10000_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latency_vs_throughput_table(filt_10000_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze 100 B events, 50,000 events/sec, various number of partitions and producers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt_50000eps_df = filter_dataframe(\n",
    "    filt_df,\n",
    "    messageSize=100,\n",
    "    producerRate=50000,\n",
    ").sort_values(['endToEndLatencyMs99Pct'], ascending=True)\n",
    "len(filt_50000eps_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "take_varying_columns(filt_50000eps_df[cols]).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Latency Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_uuid = filt_50000eps_df.iloc[0].name\n",
    "test_uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = clean_df\n",
    "t = df[df.test_uuid==test_uuid].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cumulative Distribution Function\n",
    "pubcdf = t.aggregatedPublishLatencyQuantiles\n",
    "pubcdf.name = 'Publish Latency CDF'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probability Distribution Function (latency histogram)\n",
    "pubpdf = pd.Series(index=pubcdf.index, data=np.gradient(pubcdf, pubcdf.index.values), name='Publish Latency PDF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig0, ax0 = plt.subplots()\n",
    "ax1 = ax0.twinx()\n",
    "pubpdf.plot(ax=ax0, xlim=[1,100], ylim=[0,None], style='r', title='Publish Latency PDF and CDF')\n",
    "pubcdf.plot(ax=ax1, xlim=[1,100], secondary_y=True, logx=True, ylim=[0,1])\n",
    "# ax0.set_ylabel('PDF');\n",
    "# ax1.set_ylabel('CDF');\n",
    "ax0.set_xlabel('Publish Latency (ms)');\n",
    "tick_formatter = matplotlib.ticker.LogFormatter()\n",
    "ax0.xaxis.set_major_formatter(tick_formatter)\n",
    "ax0.grid('on', which='both', axis='both')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cumulative Distribution Function\n",
    "e2ecdf = t.aggregatedEndToEndLatencyQuantiles\n",
    "e2ecdf.name = 'E2E Latency CDF'\n",
    "# Probability Distribution Function (latency histogram)\n",
    "e2epdf = pd.Series(index=e2ecdf.index, data=np.gradient(e2ecdf, e2ecdf.index.values), name='E2E Latency PDF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig0, ax0 = plt.subplots()\n",
    "ax1 = ax0.twinx()\n",
    "e2epdf.plot(ax=ax0, xlim=[1,100], ylim=[0,None], style='r', title='E2E Latency PDF and CDF')\n",
    "e2ecdf.plot(ax=ax1, xlim=[1,100], secondary_y=True, logx=True, ylim=[0,1])\n",
    "# ax0.set_ylabel('PDF');\n",
    "# ax1.set_ylabel('CDF');\n",
    "ax0.set_xlabel('E2E Latency (ms)');\n",
    "tick_formatter = matplotlib.ticker.LogFormatter()\n",
    "ax0.xaxis.set_major_formatter(tick_formatter)\n",
    "ax0.grid('on', which='both', axis='both')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined publish and E2E latency CDF\n",
    "fig0, ax0 = plt.subplots()\n",
    "xlim=[1,25]\n",
    "pubcdf.plot(ax=ax0, xlim=xlim, logx=True, ylim=[0,1], legend=True, figsize=(10,8))\n",
    "e2ecdf.plot(ax=ax0, xlim=xlim, logx=True, ylim=[0,1], legend=True)\n",
    "ax0.set_xlabel('E2E Latency (ms)');\n",
    "tick_formatter = matplotlib.ticker.LogFormatter()\n",
    "ax0.xaxis.set_major_formatter(tick_formatter)\n",
    "ax0.grid('on', which='both', axis='both')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Two Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common filter\n",
    "filt_df = filter_dataframe(\n",
    "    clean_df,\n",
    "    driverName='Pravega',\n",
    "    numWorkloadWorkers=4, \n",
    "    topics=1,\n",
    "    testDurationMinutes=15,\n",
    "    size_of_test_batch=(2,1000), # between\n",
    "    aggregatedEndToEndLatency50pct=(1,1e6),\n",
    "    messageSize=100,\n",
    "    producersPerTopic=32,\n",
    "    partitionsPerTopic=16,    \n",
    ")\n",
    "len(filt_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set 1\n",
    "filt1_df = filter_dataframe(\n",
    "    filt_df,\n",
    "    pravegaVersion='0.6.0-2361.f273314',\n",
    ")\n",
    "len(filt1_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set 2\n",
    "filt2_df = filter_dataframe(\n",
    "    filt_df,\n",
    "    pravegaVersion='0.6.0-2386.23b7340',\n",
    ")\n",
    "len(filt2_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [filt1_df, filt2_df]\n",
    "take_varying_columns(pd.concat(dfs)[param_cols]).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_latency_vs_throughput_comparison(dfs, legend_cols=None, latencyMetric='Publish'):\n",
    "    fig0, ax0 = plt.subplots()\n",
    "    cmap = plt.get_cmap('Set1')\n",
    "    colors = cmap.colors[0:len(dfs)]\n",
    "    for index, (df, color) in enumerate(zip(dfs, colors)):\n",
    "        df = df.set_index(['publishRateMBPerSecMean']).sort_index()\n",
    "        name_cols = df.iloc[0][legend_cols]\n",
    "        name = ','.join(['%s=%s' % item for item in name_cols.to_dict().items()])\n",
    "        for percentile, style in [('50',':x'), ('95','-.x'), ('99','-x')]:\n",
    "            plot_df = df[['aggregated%sLatency%spct' % (latencyMetric, percentile)]]\n",
    "            plot_df.columns = ['%s %s Latency p%s' % (name, latencyMetric, percentile)]\n",
    "            plot_df.index.name = 'Publish Throughput (MB/s)'\n",
    "            plot_df.plot(\n",
    "                ax=ax0,\n",
    "                logx=True, \n",
    "                logy=True,\n",
    "                figsize=(10,8),\n",
    "                grid=True,\n",
    "                style=style,\n",
    "                color=color,\n",
    "            )\n",
    "    ax0.set_ylabel('Latency (ms)');\n",
    "    tick_formatter = matplotlib.ticker.LogFormatter()\n",
    "    ax0.xaxis.set_major_formatter(tick_formatter)\n",
    "    ax0.yaxis.set_major_formatter(tick_formatter)\n",
    "    ax0.grid('on', which='both', axis='both')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_latency_vs_throughput_comparison([filt1_df, filt2_df], legend_cols=['pravegaVersion'], latencyMetric='Publish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_latency_vs_throughput_comparison([filt1_df, filt2_df], legend_cols=['pravegaVersion'], latencyMetric='EndToEnd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
