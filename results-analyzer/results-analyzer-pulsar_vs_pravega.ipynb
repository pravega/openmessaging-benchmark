{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install p3_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pylab as plt\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "import json\n",
    "from io import StringIO\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import p3_data\n",
    "from p3_data import (glob_file_list , load_json_from_file, merge_dicts, plot_groups, \n",
    "    get_varying_column_names, filter_dataframe, take_varying_columns,\n",
    "    load_json_records_as_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load result files from P3 Test Driver\n",
    "src_files = []\n",
    "src_files += ['../data/p3_test_driver/results/*.json']\n",
    "raw_df = load_json_records_as_dataframe(src=src_files, ignore_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean raw results\n",
    "def clean_result(result):\n",
    "    try:\n",
    "        r = result.copy()\n",
    "        r['utc_begin'] = pd.to_datetime(r['utc_begin'], utc=True)\n",
    "        r['utc_end'] = pd.to_datetime(r['utc_end'], utc=True)\n",
    "        r['git_commit'] = r['git_commit'].split(' ')[0]\n",
    "        r['driverName'] = r['driver']['name']\n",
    "        if r['driverName'] == 'Pulsar':\n",
    "            r = merge_dicts(r, r['driver']['client']['persistence'])\n",
    "        r = merge_dicts(r, r['workload'])\n",
    "        del r['workload']\n",
    "        r = merge_dicts(r, r['omb_results'])\n",
    "        if 'ansible_vars' in r and isinstance(r['ansible_vars'], dict):\n",
    "            r = merge_dicts(r, r['ansible_vars'])\n",
    "        if r['driverName'] == 'Pravega':\n",
    "            if 'pravegaVersion' not in r:\n",
    "                r['pravegaVersion'] = '0.6.0-2361.f273314-SNAPSHOT'\n",
    "            r['pravegaVersion'] = r['pravegaVersion'].replace('-SNAPSHOT','')\n",
    "        for k in list(r.keys()):\n",
    "            if 'Quantiles' in k:\n",
    "                r[k] = pd.Series(data=[float(q) for q in r[k].keys()], index=list(r[k].values())).sort_index() / 100\n",
    "            elif isinstance(r[k], list) and 'Rate' in k:\n",
    "                r[k] = pd.Series(r[k])\n",
    "                r['%sMean' % k] = r[k].mean()\n",
    "        r['numWorkloadWorkers'] = int(r.get('numWorkers', 0))\n",
    "        r['throttleEventsPerSec'] = r['producerRate']\n",
    "        r['publishRateEventsPerSecMean'] = r['publishRateMean']\n",
    "        r['publishRateMBPerSecMean'] = r['publishRateMean'] * r['messageSize'] * 1e-6\n",
    "        r['publishLatencyMsAvg'] = r['aggregatedPublishLatencyAvg']\n",
    "        r['publishLatencyMs50Pct'] = r['aggregatedPublishLatency50pct']\n",
    "        r['publishLatencyMs99Pct'] = r['aggregatedPublishLatency99pct']\n",
    "        r['endToEndLatencyMsAvg'] = r['aggregatedEndToEndLatencyAvg']\n",
    "        r['endToEndLatencyMs50Pct'] = r['aggregatedEndToEndLatency50pct']\n",
    "        r['endToEndLatencyMs99Pct'] = r['aggregatedEndToEndLatency99pct']\n",
    "        return pd.Series(r)\n",
    "    except Exception as e:\n",
    "        print('ERROR: %s: %s' % (r['test_uuid'], e))\n",
    "        # raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = raw_df.apply(clean_result, axis=1)\n",
    "clean_df = clean_df.set_index('test_uuid', drop=False)\n",
    "clean_df = clean_df[clean_df.error==False]\n",
    "clean_df = clean_df.sort_values(['utc_begin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns that identify test parameters\n",
    "param_cols = [\n",
    "    'numWorkloadWorkers',\n",
    "    'topics',\n",
    "    'partitionsPerTopic',\n",
    "    'producersPerTopic',\n",
    "    'subscriptionsPerTopic',\n",
    "    'consumerPerSubscription',\n",
    "    'testDurationMinutes',\n",
    "    'keyDistributor',\n",
    "    'git_commit',\n",
    "    'pravegaVersion',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Two Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common filter\n",
    "filt_df = filter_dataframe(\n",
    "    clean_df,\n",
    "    numWorkloadWorkers=4, \n",
    "    topics=1,\n",
    "    size_of_test_batch=(2,1000), # between\n",
    "    aggregatedEndToEndLatency50pct=(1,1e6),\n",
    "    messageSize=10000,\n",
    ")\n",
    "len(filt_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set 1\n",
    "filt1_df = filter_dataframe(\n",
    "    filt_df,\n",
    "    driverName='Pravega',\n",
    "    pravegaVersion='0.6.0-2386.23b7340',\n",
    "#     producersPerTopic=32,\n",
    "#     partitionsPerTopic=16,    \n",
    "    testDurationMinutes=15,\n",
    ")\n",
    "len(filt1_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set 2\n",
    "filt2_df = filter_dataframe(\n",
    "    filt_df,\n",
    "    driverName='Pulsar',\n",
    "    producersPerTopic=4,\n",
    "    partitionsPerTopic=16,\n",
    "    deduplicationEnabled=True,\n",
    "    testDurationMinutes=5,    \n",
    ")\n",
    "len(filt2_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View varying columns\n",
    "dfs = [filt1_df, filt2_df]\n",
    "take_varying_columns(pd.concat(dfs)[param_cols]).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "legend_cols=['driverName']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_latency_vs_throughput_comparison(dfs, legend_cols=None, latencyMetric='Publish'):\n",
    "    fig0, ax0 = plt.subplots()\n",
    "    cmap = plt.get_cmap('Set1')\n",
    "    colors = cmap.colors[0:len(dfs)]\n",
    "    for index, (df, color) in enumerate(zip(dfs, colors)):\n",
    "        df = df.set_index(['publishRateMBPerSecMean']).sort_index()\n",
    "        name_cols = df.iloc[0][legend_cols]\n",
    "        name = ','.join(['%s=%s' % item for item in name_cols.to_dict().items()])\n",
    "        for percentile, style in [('50',':x'), ('95','-.x'), ('99','-x')]:\n",
    "            plot_df = df[['aggregated%sLatency%spct' % (latencyMetric, percentile)]]\n",
    "            plot_df.columns = ['%s %s Latency p%s' % (name, latencyMetric, percentile)]\n",
    "            plot_df.index.name = 'Publish Throughput (MB/s)'\n",
    "            plot_df.plot(\n",
    "                ax=ax0,\n",
    "                logx=True, \n",
    "                logy=True,\n",
    "                figsize=(10,8),\n",
    "                grid=True,\n",
    "                style=style,\n",
    "                color=color,\n",
    "            )\n",
    "    ax0.set_ylabel('Latency (ms)');\n",
    "    tick_formatter = matplotlib.ticker.LogFormatter()\n",
    "    ax0.xaxis.set_major_formatter(tick_formatter)\n",
    "    ax0.yaxis.set_major_formatter(tick_formatter)\n",
    "    ax0.grid('on', which='both', axis='both')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_latency_vs_throughput_comparison(dfs, legend_cols=legend_cols, latencyMetric='Publish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_latency_vs_throughput_comparison(dfs, legend_cols=legend_cols, latencyMetric='EndToEnd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
